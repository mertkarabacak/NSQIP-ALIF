{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxtJ9G-aIZ_y"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_PnaQ4NIuE0"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install shap\n",
        "!pip install pytorch-tabnet\n",
        "!pip install tabpfn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrZMQLTfIQPb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score, auc, precision_score, recall_score, accuracy_score, balanced_accuracy_score, brier_score_loss\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from math import sqrt\n",
        "from scipy import stats as st\n",
        "from random import randrange\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from tabpfn import TabPFNClassifier\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "import shap\n",
        "from sklearn.inspection import PartialDependenceDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWbpPFMVHS-V"
      },
      "source": [
        "# Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDsi_6tnIQPc"
      },
      "outputs": [],
      "source": [
        "#Open csv file.\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/NSQIP-ALIF/final_data.csv', index_col = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPmUaNyUN2p8"
      },
      "outputs": [],
      "source": [
        "#Define outcome of interest.\n",
        "\n",
        "data.loc[data['Discharge Destination'] == 'Home', 'OUTCOME'] = 0\n",
        "data.loc[data['Discharge Destination'] == 'Facility Which was Home', 'OUTCOME'] = 0\n",
        "data.loc[data['Discharge Destination'] == 'Rehab', 'OUTCOME'] = 1\n",
        "data.loc[data['Discharge Destination'] == 'Skilled Care, Not Home', 'OUTCOME'] = 1\n",
        "data.loc[data['Discharge Destination'] == 'Separate Acute Care', 'OUTCOME'] = 1\n",
        "data.loc[data['Discharge Destination'] == 'Multi-level Senior Community', 'OUTCOME'] = 1\n",
        "data.loc[data['Discharge Destination'] == 'Unskilled Facility Not Home', 'OUTCOME'] = 1\n",
        "\n",
        "print(data['OUTCOME'].value_counts(normalize=False, dropna=False))\n",
        "print(data['OUTCOME'].value_counts(normalize=True, dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhZeK6d8IQPd"
      },
      "outputs": [],
      "source": [
        "#Define predictor variables (x) and outcome of interest (y).\n",
        "\n",
        "outcomes = ['Discharge Destination', 'Readmission Status', 'Reoperation Status', 'Length of Stay', 'Complications', 'OUTCOME']\n",
        "\n",
        "x = data.drop(outcomes, axis = 1)\n",
        "y = data['OUTCOME']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mt6ks1daIQPd"
      },
      "outputs": [],
      "source": [
        "#Check data shapes.\n",
        "\n",
        "print(y.shape)\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0IwUyCsIQPe"
      },
      "outputs": [],
      "source": [
        "#Split data into initial train set and test set in 80:20 ratio.\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
        "\n",
        "#Describe initial train set and test set.\n",
        "\n",
        "print(\"Number patients x_train dataset: \", x_train.shape[0])\n",
        "print(\"Number patients y_train dataset: \", y_train.shape[0])\n",
        "print(\"Number patients x_test dataset: \", x_test.shape[0])\n",
        "print(\"Number patients y_test dataset: \", y_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyNo7C2Pbzt2"
      },
      "outputs": [],
      "source": [
        "#Split initial train set into final train set and validation set in 75:25 ratio.\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(x_train, y_train, test_size = 0.25, random_state = 0)\n",
        "\n",
        "#Describe train and validation sets.\n",
        "\n",
        "print(\"Number patients train_x dataset: \", train_x.shape[0])\n",
        "print(\"Number patients train_y dataset: \", train_y.shape[0])\n",
        "print(\"Number patients valid_x dataset: \", valid_x.shape[0])\n",
        "print(\"Number patients valid_y dataset: \", valid_y.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2O0He35IQPe"
      },
      "outputs": [],
      "source": [
        "#Describe outcome of interest before resampling.\n",
        "\n",
        "print(\"Before resampling, counts of label '1': {}\".format(sum(train_y == 1)))\n",
        "print(\"Before resampling, counts of label '0': {} \\n\".format(sum(train_y == 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvXczViaIQPe"
      },
      "outputs": [],
      "source": [
        "#Apply SMOTE.\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "resampler = SMOTE(random_state = 0)\n",
        "train_x, train_y = resampler.fit_resample(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYw68tzFIQPe"
      },
      "outputs": [],
      "source": [
        "#Describe outcome of interest after resampling.\n",
        "\n",
        "print(\"After resampling, counts of label '1': {}\".format(sum(train_y == 1)))\n",
        "print(\"After resampling, counts of label '0': {} \\n\".format(sum(train_y == 0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrb8PN0dEOc1"
      },
      "outputs": [],
      "source": [
        "#Define function for AUROC with 95% confidence intervals.\n",
        "\n",
        "def auroc_ci(y_test, y_probs, positive=1, n_samples=1000, alpha=0.05):\n",
        "    auroc_values = []\n",
        "    for i in range(n_samples):\n",
        "        y_test_bs, y_probs_bs = resample(y_test, y_probs)\n",
        "\n",
        "        auroc = roc_auc_score(y_test_bs, y_probs_bs)\n",
        "        auroc_values.append(auroc)\n",
        "\n",
        "    mean_auroc = np.mean(auroc_values)\n",
        "    std_auroc = np.std(auroc_values)\n",
        "\n",
        "    lower_ci = mean_auroc - 1.96 * std_auroc\n",
        "    upper_ci = mean_auroc + 1.96 * std_auroc\n",
        "\n",
        "    return auroc, lower_ci, upper_ci"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLkyBrpf95-0"
      },
      "outputs": [],
      "source": [
        "#Define z-value for other confidence intervals.\n",
        "\n",
        "confidence = 0.95\n",
        "z_value = st.norm.ppf((1 + confidence) / 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWX_CriXYlL_"
      },
      "source": [
        "#TabPFN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFQmwskRLrY4"
      },
      "source": [
        "No hyperparameter tuning is performed for TabPFN since the paper that introduced TabPFN claims no hyperparameter tuning is needed for it:\n",
        "\n",
        "\n",
        "*We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods.*\n",
        "\n",
        "https://doi.org/10.48550/arXiv.2207.01848\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3gvvODJ8B2Z"
      },
      "outputs": [],
      "source": [
        "#Fit TabPFN.\n",
        "\n",
        "tabpfn = TabPFNClassifier(device='cpu', N_ensemble_configurations=2)\n",
        "\n",
        "tabpfn.fit(x_train.sample(n=1024, random_state=42), y_train.sample(n=1024, random_state=42), overwrite_warning = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oLbONin8YGA"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_tabpfn = tabpfn.predict_proba(valid_x.values)\n",
        "\n",
        "calib_model_tabpfn = LogisticRegression()\n",
        "calib_model_tabpfn.fit(calib_probs_tabpfn, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZNInXD39Tux"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained TabNet model.\n",
        "\n",
        "preds_tabpfn = tabpfn.predict(x_test.values)\n",
        "\n",
        "uncalibrated_probs_tabpfn = tabpfn.predict_proba(x_test.values)\n",
        "\n",
        "probs_tabpfn = calib_model_tabpfn.predict_proba(uncalibrated_probs_tabpfn)\n",
        "probs_tabpfn = probs_tabpfn[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm8PNeSm9dtI"
      },
      "outputs": [],
      "source": [
        "#Evaluate TabNet model.\n",
        "\n",
        "tabpfn_precision = round(precision_score(y_test, preds_tabpfn, average = 'weighted'), 3)\n",
        "tabpfn_precision_ci_length = z_value * np.sqrt((tabpfn_precision * (1 - tabpfn_precision)) / y_test.shape[0])\n",
        "tabpfn_precision_ci_lower = round((tabpfn_precision - tabpfn_precision_ci_length), 3)\n",
        "tabpfn_precision_ci_upper = round((tabpfn_precision + tabpfn_precision_ci_length), 3)\n",
        "tabpfn_precision_str = str(tabpfn_precision) + ' (' + str(tabpfn_precision_ci_lower) + ' - ' + str(tabpfn_precision_ci_upper) + ')'\n",
        "\n",
        "tabpfn_recall = round(recall_score(y_test, preds_tabpfn, average = 'weighted'), 3)\n",
        "tabpfn_recall_ci_length = z_value * np.sqrt((tabpfn_recall * (1 - tabpfn_recall)) / y_test.shape[0])\n",
        "tabpfn_recall_ci_lower = round((tabpfn_recall - tabpfn_recall_ci_length), 3)\n",
        "tabpfn_recall_ci_upper = round((tabpfn_recall + tabpfn_recall_ci_length), 3)\n",
        "tabpfn_recall_str = str(tabpfn_recall) + ' (' + str(tabpfn_recall_ci_lower) + ' - ' + str(tabpfn_recall_ci_upper) + ')'\n",
        "\n",
        "tabpfn_auprc = round(average_precision_score(y_test, probs_tabpfn, average = 'weighted'), 3)\n",
        "tabpfn_auprc_ci_length = z_value * np.sqrt((tabpfn_auprc * (1 - tabpfn_auprc)) / y_test.shape[0])\n",
        "tabpfn_auprc_ci_lower = round((tabpfn_auprc - tabpfn_auprc_ci_length), 3)\n",
        "tabpfn_auprc_ci_upper = round((tabpfn_auprc + tabpfn_auprc_ci_length), 3)\n",
        "tabpfn_auprc_str = str(tabpfn_auprc) + ' (' + str(tabpfn_auprc_ci_lower) + ' - ' + str(tabpfn_auprc_ci_upper) + ')'\n",
        "\n",
        "tabpfn_accuracy = round(balanced_accuracy_score(y_test, preds_tabpfn), 3)\n",
        "tabpfn_accuracy_ci_length = z_value * np.sqrt((tabpfn_accuracy * (1 - tabpfn_accuracy)) / y_test.shape[0])\n",
        "tabpfn_accuracy_ci_lower = round((tabpfn_accuracy - tabpfn_accuracy_ci_length), 3)\n",
        "tabpfn_accuracy_ci_upper = round((tabpfn_accuracy + tabpfn_accuracy_ci_length), 3)\n",
        "tabpfn_accuracy_str = str(tabpfn_accuracy) + ' (' + str(tabpfn_accuracy_ci_lower) + ' - ' + str(tabpfn_accuracy_ci_upper) + ')'\n",
        "\n",
        "tabpfn_auroc, tabpfn_auroc_ci_lower, tabpfn_auroc_ci_upper = auroc_ci(y_test, probs_tabpfn)\n",
        "tabpfn_auroc = round(tabpfn_auroc, 3)\n",
        "tabpfn_auroc_ci_lower = round(tabpfn_auroc_ci_lower, 3)\n",
        "tabpfn_auroc_ci_upper = round(tabpfn_auroc_ci_upper, 3)\n",
        "tabpfn_auroc_str = str(tabpfn_auroc) + ' (' + str(tabpfn_auroc_ci_lower) + ' - ' + str(tabpfn_auroc_ci_upper) + ')'\n",
        "\n",
        "tabpfn_brier = round(brier_score_loss(y_test, probs_tabpfn), 3)\n",
        "tabpfn_brier_ci_length = z_value * np.sqrt((tabpfn_brier * (1 - tabpfn_brier)) / y_test.shape[0])\n",
        "tabpfn_brier_ci_lower = round((tabpfn_brier - tabpfn_brier_ci_length), 3)\n",
        "tabpfn_brier_ci_upper = round((tabpfn_brier + tabpfn_brier_ci_length), 3)\n",
        "tabpfn_brier_str = str(tabpfn_brier) + ' (' + str(tabpfn_brier_ci_lower) + ' - ' + str(tabpfn_brier_ci_upper) + ')'\n",
        "\n",
        "tabpfn_results = [tabpfn_precision_str, tabpfn_recall_str, tabpfn_auprc_str, tabpfn_accuracy_str, tabpfn_auroc_str, tabpfn_brier_str]\n",
        "\n",
        "print(\"Precision: \", (tabpfn_precision_str))\n",
        "print(\"Recall: \", (tabpfn_recall_str))\n",
        "print('AUPRC: ', (tabpfn_auprc_str))\n",
        "print('Accuracy: ', (tabpfn_accuracy_str))\n",
        "print('AUROC: ', (tabpfn_auroc_str))\n",
        "print('Brier Score: ', (tabpfn_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wp5veHVFWIB"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "tabpfn_precision_curve, tabpfn_recall_curve, _ = precision_recall_curve(y_test, probs_tabpfn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY44IPIU7sBi"
      },
      "source": [
        "# TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beooviDI7uuS"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for TabNet.\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    param = {\n",
        "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
        "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
        "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
        "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
        "        \"lambda_sparse\": trial.suggest_loguniform(\"lambda_sparse\", 1e-6, 1e-3),\n",
        "        \"optimizer_params\": {\"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)},\n",
        "        \"seed\": 31,\n",
        "    }\n",
        "\n",
        "    tabnet_clf = TabNetClassifier(**param)\n",
        "\n",
        "    tabnet_clf.fit(\n",
        "        train_x.values,\n",
        "        train_y.values,\n",
        "        eval_set=[(valid_x.values, valid_y.values)],\n",
        "        eval_metric=[\"auc\"],\n",
        "        max_epochs=10,\n",
        "        patience=20,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    preds = tabnet_clf.predict_proba(valid_x.values)[:, 1]\n",
        "    pred_labels = np.rint(preds)\n",
        "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=31))\n",
        "    study.optimize(objective, n_trials=10, timeout=600)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    tabnet_params = {}\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        tabnet_params[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXsTkX3n_i2y"
      },
      "outputs": [],
      "source": [
        "#See TabNet hyperparameters.\n",
        "\n",
        "tabnet_params.pop('lr')\n",
        "\n",
        "print(tabnet_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pujw5iMQ_i2y"
      },
      "outputs": [],
      "source": [
        "#Fit TabNet.\n",
        "\n",
        "tabnet = TabNetClassifier(**tabnet_params)\n",
        "\n",
        "tabnet.fit(train_x.values, train_y.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYiPND2v_i2y"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_tabnet = tabnet.predict_proba(valid_x.values)\n",
        "\n",
        "calib_model_tabnet = LogisticRegression()\n",
        "calib_model_tabnet.fit(calib_probs_tabnet, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1AJq9eA_i2y"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained TabNet model.\n",
        "\n",
        "preds_tabnet = tabnet.predict(x_test.values)\n",
        "\n",
        "uncalibrated_probs_tabnet = tabnet.predict_proba(x_test.values)\n",
        "\n",
        "probs_tabnet = calib_model_tabnet.predict_proba(uncalibrated_probs_tabnet)\n",
        "probs_tabnet = probs_tabnet[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP-y4lEI_i2z"
      },
      "outputs": [],
      "source": [
        "#Evaluate TabNet model.\n",
        "\n",
        "tabnet_precision = round(precision_score(y_test, preds_tabnet, average = 'weighted'), 3)\n",
        "tabnet_precision_ci_length = z_value * np.sqrt((tabnet_precision * (1 - tabnet_precision)) / y_test.shape[0])\n",
        "tabnet_precision_ci_lower = round((tabnet_precision - tabnet_precision_ci_length), 3)\n",
        "tabnet_precision_ci_upper = round((tabnet_precision + tabnet_precision_ci_length), 3)\n",
        "tabnet_precision_str = str(tabnet_precision) + ' (' + str(tabnet_precision_ci_lower) + ' - ' + str(tabnet_precision_ci_upper) + ')'\n",
        "\n",
        "tabnet_recall = round(recall_score(y_test, preds_tabnet, average = 'weighted'), 3)\n",
        "tabnet_recall_ci_length = z_value * np.sqrt((tabnet_recall * (1 - tabnet_recall)) / y_test.shape[0])\n",
        "tabnet_recall_ci_lower = round((tabnet_recall - tabnet_recall_ci_length), 3)\n",
        "tabnet_recall_ci_upper = round((tabnet_recall + tabnet_recall_ci_length), 3)\n",
        "tabnet_recall_str = str(tabnet_recall) + ' (' + str(tabnet_recall_ci_lower) + ' - ' + str(tabnet_recall_ci_upper) + ')'\n",
        "\n",
        "tabnet_auprc = round(average_precision_score(y_test, probs_tabnet, average = 'weighted'), 3)\n",
        "tabnet_auprc_ci_length = z_value * np.sqrt((tabnet_auprc * (1 - tabnet_auprc)) / y_test.shape[0])\n",
        "tabnet_auprc_ci_lower = round((tabnet_auprc - tabnet_auprc_ci_length), 3)\n",
        "tabnet_auprc_ci_upper = round((tabnet_auprc + tabnet_auprc_ci_length), 3)\n",
        "tabnet_auprc_str = str(tabnet_auprc) + ' (' + str(tabnet_auprc_ci_lower) + ' - ' + str(tabnet_auprc_ci_upper) + ')'\n",
        "\n",
        "tabnet_accuracy = round(balanced_accuracy_score(y_test, preds_tabnet), 3)\n",
        "tabnet_accuracy_ci_length = z_value * np.sqrt((tabnet_accuracy * (1 - tabnet_accuracy)) / y_test.shape[0])\n",
        "tabnet_accuracy_ci_lower = round((tabnet_accuracy - tabnet_accuracy_ci_length), 3)\n",
        "tabnet_accuracy_ci_upper = round((tabnet_accuracy + tabnet_accuracy_ci_length), 3)\n",
        "tabnet_accuracy_str = str(tabnet_accuracy) + ' (' + str(tabnet_accuracy_ci_lower) + ' - ' + str(tabnet_accuracy_ci_upper) + ')'\n",
        "\n",
        "tabnet_auroc, tabnet_auroc_ci_lower, tabnet_auroc_ci_upper = auroc_ci(y_test, probs_tabnet)\n",
        "tabnet_auroc = round(tabnet_auroc, 3)\n",
        "tabnet_auroc_ci_lower = round(tabnet_auroc_ci_lower, 3)\n",
        "tabnet_auroc_ci_upper = round(tabnet_auroc_ci_upper, 3)\n",
        "tabnet_auroc_str = str(tabnet_auroc) + ' (' + str(tabnet_auroc_ci_lower) + ' - ' + str(tabnet_auroc_ci_upper) + ')'\n",
        "\n",
        "tabnet_brier = round(brier_score_loss(y_test, probs_tabnet), 3)\n",
        "tabnet_brier_ci_length = z_value * np.sqrt((tabnet_brier * (1 - tabnet_brier)) / y_test.shape[0])\n",
        "tabnet_brier_ci_lower = round((tabnet_brier - tabnet_brier_ci_length), 3)\n",
        "tabnet_brier_ci_upper = round((tabnet_brier + tabnet_brier_ci_length), 3)\n",
        "tabnet_brier_str = str(tabnet_brier) + ' (' + str(tabnet_brier_ci_lower) + ' - ' + str(tabnet_brier_ci_upper) + ')'\n",
        "\n",
        "tabnet_results = [tabnet_precision_str, tabnet_recall_str, tabnet_auprc_str, tabnet_accuracy_str, tabnet_auroc_str, tabnet_brier_str]\n",
        "\n",
        "print(\"Precision: \", (tabnet_precision_str))\n",
        "print(\"Recall: \", (tabnet_recall_str))\n",
        "print('AUPRC: ', (tabnet_auprc_str))\n",
        "print('Accuracy: ', (tabnet_accuracy_str))\n",
        "print('AUROC: ', (tabnet_auroc_str))\n",
        "print('Brier Score: ', (tabnet_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsAsKQ1e_i2z"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "tabnet_precision_curve, tabnet_recall_curve, _ = precision_recall_curve(y_test, probs_tabnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esuV5nR8IQPe"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBWNs_MxIQPf"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter tuning for XGBoost.\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    dtrain = xgb.DMatrix(train_x, label=train_y)\n",
        "    dvalid = xgb.DMatrix(valid_x, label=valid_y)\n",
        "\n",
        "    param = {\n",
        "        \"seed\": 31,\n",
        "        \"verbosity\": 0,\n",
        "        \"objective\":  trial.suggest_categorical(\"objective\", [\"binary:logistic\"]),\n",
        "        \"eval_metric\": \"auc\",\n",
        "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\"]),\n",
        "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
        "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
        "        \"max_depth\" : trial.suggest_int(\"max_depth\", 1, 9),\n",
        "        \"eta\" : trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n",
        "        \"gamma\" : trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n",
        "        \"grow_policy\" : trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
        "    }\n",
        "\n",
        "\n",
        "    # Add a callback for pruning.\n",
        "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation-auc\")\n",
        "\n",
        "    bst = xgb.train(param, dtrain, evals=[(dvalid, \"validation\")], callbacks=[pruning_callback])\n",
        "    preds = bst.predict(dvalid)\n",
        "    pred_labels = np.rint(preds)\n",
        "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\", sampler = TPESampler(seed=31)\n",
        "    )\n",
        "    study.optimize(objective, n_trials=100)\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    xgb_params = {}\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        xgb_params[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "algcmJCqkAya"
      },
      "outputs": [],
      "source": [
        "#See XGBoost hyperparameters.\n",
        "\n",
        "xgb_params['eval_metric'] = 'auc'\n",
        "xgb_params['verbosity'] = 0\n",
        "xgb_params['seed'] = 31\n",
        "\n",
        "print(xgb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZk8hbTUIQPg"
      },
      "outputs": [],
      "source": [
        "#Fit XGBoost.\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "xgb = XGBClassifier(**xgb_params)\n",
        "\n",
        "xgb.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUf-OtCoIQPg"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_xgb = xgb.predict_proba(valid_x)\n",
        "\n",
        "calib_model_xgb = LogisticRegression()\n",
        "calib_model_xgb.fit(calib_probs_xgb, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq-OU3c29-4L"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained and calibrated XGBoost model.\n",
        "\n",
        "preds_xgb = xgb.predict(x_test)\n",
        "\n",
        "uncalibrated_probs_xgb = xgb.predict_proba(x_test)\n",
        "\n",
        "probs_xgb = calib_model_xgb.predict_proba(uncalibrated_probs_xgb)\n",
        "probs_xgb = probs_xgb[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5m3AkfQJIQPg"
      },
      "outputs": [],
      "source": [
        "#Evaluate XGBoost model.\n",
        "\n",
        "xgb_precision = round(precision_score(y_test, preds_xgb, average = 'weighted'), 3)\n",
        "xgb_precision_ci_length = z_value * np.sqrt((xgb_precision * (1 - xgb_precision)) / y_test.shape[0])\n",
        "xgb_precision_ci_lower = round((xgb_precision - xgb_precision_ci_length), 3)\n",
        "xgb_precision_ci_upper = round((xgb_precision + xgb_precision_ci_length), 3)\n",
        "xgb_precision_str = str(xgb_precision) + ' (' + str(xgb_precision_ci_lower) + ' - ' + str(xgb_precision_ci_upper) + ')'\n",
        "\n",
        "xgb_recall = round(recall_score(y_test, preds_xgb, average = 'weighted'), 3)\n",
        "xgb_recall_ci_length = z_value * np.sqrt((xgb_recall * (1 - xgb_recall)) / y_test.shape[0])\n",
        "xgb_recall_ci_lower = round((xgb_recall - xgb_recall_ci_length), 3)\n",
        "xgb_recall_ci_upper = round((xgb_recall + xgb_recall_ci_length), 3)\n",
        "xgb_recall_str = str(xgb_recall) + ' (' + str(xgb_recall_ci_lower) + ' - ' + str(xgb_recall_ci_upper) + ')'\n",
        "\n",
        "xgb_auprc = round(average_precision_score(y_test, probs_xgb, average = 'weighted'), 3)\n",
        "xgb_auprc_ci_length = z_value * np.sqrt((xgb_auprc * (1 - xgb_auprc)) / y_test.shape[0])\n",
        "xgb_auprc_ci_lower = round((xgb_auprc - xgb_auprc_ci_length), 3)\n",
        "xgb_auprc_ci_upper = round((xgb_auprc + xgb_auprc_ci_length), 3)\n",
        "xgb_auprc_str = str(xgb_auprc) + ' (' + str(xgb_auprc_ci_lower) + ' - ' + str(xgb_auprc_ci_upper) + ')'\n",
        "\n",
        "xgb_accuracy = round(balanced_accuracy_score(y_test, preds_xgb), 3)\n",
        "xgb_accuracy_ci_length = z_value * np.sqrt((xgb_accuracy * (1 - xgb_accuracy)) / y_test.shape[0])\n",
        "xgb_accuracy_ci_lower = round((xgb_accuracy - xgb_accuracy_ci_length), 3)\n",
        "xgb_accuracy_ci_upper = round((xgb_accuracy + xgb_accuracy_ci_length), 3)\n",
        "xgb_accuracy_str = str(xgb_accuracy) + ' (' + str(xgb_accuracy_ci_lower) + ' - ' + str(xgb_accuracy_ci_upper) + ')'\n",
        "\n",
        "xgb_auroc, xgb_auroc_ci_lower, xgb_auroc_ci_upper = auroc_ci(y_test, probs_xgb)\n",
        "xgb_auroc = round(xgb_auroc, 3)\n",
        "xgb_auroc_ci_lower = round(xgb_auroc_ci_lower, 3)\n",
        "xgb_auroc_ci_upper = round(xgb_auroc_ci_upper, 3)\n",
        "xgb_auroc_str = str(xgb_auroc) + ' (' + str(xgb_auroc_ci_lower) + ' - ' + str(xgb_auroc_ci_upper) + ')'\n",
        "\n",
        "xgb_brier = round(brier_score_loss(y_test, probs_xgb), 3)\n",
        "xgb_brier_ci_length = z_value * np.sqrt((xgb_brier * (1 - xgb_brier)) / y_test.shape[0])\n",
        "xgb_brier_ci_lower = round((xgb_brier - xgb_brier_ci_length), 3)\n",
        "xgb_brier_ci_upper = round((xgb_brier + xgb_brier_ci_length), 3)\n",
        "xgb_brier_str = str(xgb_brier) + ' (' + str(xgb_brier_ci_lower) + ' - ' + str(xgb_brier_ci_upper) + ')'\n",
        "\n",
        "xgb_results = [xgb_precision_str, xgb_recall_str, xgb_auprc_str, xgb_accuracy_str, xgb_auroc_str, xgb_brier_str]\n",
        "\n",
        "print(\"Precision: \", (xgb_precision_str))\n",
        "print(\"Recall: \", (xgb_recall_str))\n",
        "print('AUPRC: ', (xgb_auprc_str))\n",
        "print('Accuracy: ', (xgb_accuracy_str))\n",
        "print('AUROC: ', (xgb_auroc_str))\n",
        "print('Brier Score: ', (xgb_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ch-wpxcQIQPg"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "xgb_precision_curve, xgb_recall_curve, _ = precision_recall_curve(y_test, probs_xgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VszU3k5IQPh"
      },
      "source": [
        "# LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR81p9HpIQPh"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter tuning for LightGBM.\n",
        "\n",
        "def objective(trial):\n",
        "    dtrain = lgb.Dataset(train_x, label=train_y)\n",
        "\n",
        "    param = {\n",
        "        \"objective\":  trial.suggest_categorical(\"objective\", [\"binary\"]),\n",
        "        \"metric\": \"binary_logloss\",\n",
        "        \"verbosity\": -1,\n",
        "        \"random_state\": 31,\n",
        "        \"boosting_type\":  trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n",
        "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
        "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
        "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
        "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
        "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
        "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
        "    }\n",
        "\n",
        "    gbm = lgb.train(param, dtrain)\n",
        "    preds = gbm.predict(valid_x)\n",
        "    pred_labels = np.rint(preds)\n",
        "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction=\"maximize\", sampler = TPESampler(seed=31))\n",
        "    study.optimize(objective, n_trials=100)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    lgb_params = {}\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        lgb_params[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7P7Mw7UkC-y"
      },
      "outputs": [],
      "source": [
        "#See LightGBM hyperparameters.\n",
        "\n",
        "lgb_params['metric'] = 'binary_logloss'\n",
        "lgb_params['verbosity'] = -1\n",
        "lgb_params['random_state'] = 31\n",
        "\n",
        "print(lgb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAidUSs1IQPh"
      },
      "outputs": [],
      "source": [
        "#Fit LightGBM.\n",
        "\n",
        "import lightgbm as lgb\n",
        "\n",
        "lgb = lgb.LGBMClassifier(**lgb_params)\n",
        "\n",
        "lgb.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlKe1z0RIQPh"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_lgb = lgb.predict_proba(valid_x)\n",
        "\n",
        "calib_model_lgb = LogisticRegression()\n",
        "calib_model_lgb.fit(calib_probs_lgb, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPyYlwGX-HkZ"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained and calibrated LightGBM model.\n",
        "\n",
        "preds_lgb = lgb.predict(x_test)\n",
        "\n",
        "uncalibrated_probs_lgb = lgb.predict_proba(x_test)\n",
        "uncalibrated_probs_lgb = uncalibrated_probs_lgb\n",
        "\n",
        "probs_lgb = calib_model_lgb.predict_proba(uncalibrated_probs_lgb)\n",
        "probs_lgb = probs_lgb[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r259oDdIIQPi"
      },
      "outputs": [],
      "source": [
        "#Evaluate LightGBM model.\n",
        "\n",
        "lgb_precision = round(precision_score(y_test, preds_lgb, average = 'weighted'), 3)\n",
        "lgb_precision_ci_length = z_value * np.sqrt((lgb_precision * (1 - lgb_precision)) / y_test.shape[0])\n",
        "lgb_precision_ci_lower = round((lgb_precision - lgb_precision_ci_length), 3)\n",
        "lgb_precision_ci_upper = round((lgb_precision + lgb_precision_ci_length), 3)\n",
        "lgb_precision_str = str(lgb_precision) + ' (' + str(lgb_precision_ci_lower) + ' - ' + str(lgb_precision_ci_upper) + ')'\n",
        "\n",
        "lgb_recall = round(recall_score(y_test, preds_lgb, average = 'weighted'), 3)\n",
        "lgb_recall_ci_length = z_value * np.sqrt((lgb_recall * (1 - lgb_recall)) / y_test.shape[0])\n",
        "lgb_recall_ci_lower = round((lgb_recall - lgb_recall_ci_length), 3)\n",
        "lgb_recall_ci_upper = round((lgb_recall + lgb_recall_ci_length), 3)\n",
        "lgb_recall_str = str(lgb_recall) + ' (' + str(lgb_recall_ci_lower) + ' - ' + str(lgb_recall_ci_upper) + ')'\n",
        "\n",
        "lgb_auprc = round(average_precision_score(y_test, probs_lgb, average = 'weighted'), 3)\n",
        "lgb_auprc_ci_length = z_value * np.sqrt((lgb_auprc * (1 - lgb_auprc)) / y_test.shape[0])\n",
        "lgb_auprc_ci_lower = round((lgb_auprc - lgb_auprc_ci_length), 3)\n",
        "lgb_auprc_ci_upper = round((lgb_auprc + lgb_auprc_ci_length), 3)\n",
        "lgb_auprc_str = str(lgb_auprc) + ' (' + str(lgb_auprc_ci_lower) + ' - ' + str(lgb_auprc_ci_upper) + ')'\n",
        "\n",
        "lgb_accuracy = round(balanced_accuracy_score(y_test, preds_lgb), 3)\n",
        "lgb_accuracy_ci_length = z_value * np.sqrt((lgb_accuracy * (1 - lgb_accuracy)) / y_test.shape[0])\n",
        "lgb_accuracy_ci_lower = round((lgb_accuracy - lgb_accuracy_ci_length), 3)\n",
        "lgb_accuracy_ci_upper = round((lgb_accuracy + lgb_accuracy_ci_length), 3)\n",
        "lgb_accuracy_str = str(lgb_accuracy) + ' (' + str(lgb_accuracy_ci_lower) + ' - ' + str(lgb_accuracy_ci_upper) + ')'\n",
        "\n",
        "lgb_auroc, lgb_auroc_ci_lower, lgb_auroc_ci_upper = auroc_ci(y_test, probs_lgb)\n",
        "lgb_auroc = round(lgb_auroc, 3)\n",
        "lgb_auroc_ci_lower = round(lgb_auroc_ci_lower, 3)\n",
        "lgb_auroc_ci_upper = round(lgb_auroc_ci_upper, 3)\n",
        "lgb_auroc_str = str(lgb_auroc) + ' (' + str(lgb_auroc_ci_lower) + ' - ' + str(lgb_auroc_ci_upper) + ')'\n",
        "\n",
        "lgb_brier = round(brier_score_loss(y_test, probs_lgb), 3)\n",
        "lgb_brier_ci_length = z_value * np.sqrt((lgb_brier * (1 - lgb_brier)) / y_test.shape[0])\n",
        "lgb_brier_ci_lower = round((lgb_brier - lgb_brier_ci_length), 3)\n",
        "lgb_brier_ci_upper = round((lgb_brier + lgb_brier_ci_length), 3)\n",
        "lgb_brier_str = str(lgb_brier) + ' (' + str(lgb_brier_ci_lower) + ' - ' + str(lgb_brier_ci_upper) + ')'\n",
        "\n",
        "lgb_results = [lgb_precision_str, lgb_recall_str, lgb_auprc_str, lgb_accuracy_str, lgb_auroc_str, lgb_brier_str]\n",
        "\n",
        "print(\"Precision: \", (lgb_precision_str))\n",
        "print(\"Recall: \", (lgb_recall_str))\n",
        "print('AUPRC: ', (lgb_auprc_str))\n",
        "print('Accuracy: ', (lgb_accuracy_str))\n",
        "print('AUROC: ', (lgb_auroc_str))\n",
        "print('Brier Score: ', (lgb_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXzXI9O-IQPi"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "lgb_precision_curve, lgb_recall_curve, _ = precision_recall_curve(y_test, probs_lgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQRRrBp-IQPk"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFTmQMQxIQPk"
      },
      "outputs": [],
      "source": [
        "#Hyperparameter tuning for Random Forest.\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    param = {\n",
        "        \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"]),\n",
        "        \"random_state\": 31,\n",
        "        \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\",\"log2\", None]),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100),\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 2000, 100),\n",
        "        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4, 1),\n",
        "        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10, 1),\n",
        "    }\n",
        "\n",
        "    rf = RandomForestClassifier(**param)\n",
        "\n",
        "    rf.fit(\n",
        "        train_x,\n",
        "        train_y,\n",
        "    )\n",
        "\n",
        "    preds = rf.predict(valid_x)\n",
        "    pred_labels = np.rint(preds)\n",
        "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=31))\n",
        "    study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    rf_params = {}\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        rf_params[key] = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTqnQTzgkG3R"
      },
      "outputs": [],
      "source": [
        "#See Random Forest hyperparameters.\n",
        "\n",
        "rf_params['random_state'] = 31\n",
        "\n",
        "print(rf_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2nnbDAsIQPk"
      },
      "outputs": [],
      "source": [
        "#Fit Random Forest.\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf = RandomForestClassifier(**rf_params)\n",
        "\n",
        "rf.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSsBliFuIQPk"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_rf = rf.predict_proba(valid_x)\n",
        "\n",
        "calib_model_rf = LogisticRegression()\n",
        "calib_model_rf.fit(calib_probs_rf, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3Ru1zLF-YER"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained Random Forest model.\n",
        "\n",
        "preds_rf = rf.predict(x_test)\n",
        "\n",
        "uncalibrated_probs_rf = rf.predict_proba(x_test)\n",
        "\n",
        "probs_rf = calib_model_rf.predict_proba(uncalibrated_probs_rf)\n",
        "probs_rf = probs_rf[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haQcIGjoIQPk"
      },
      "outputs": [],
      "source": [
        "#Evaluate Random Forest model.\n",
        "\n",
        "rf_precision = round(precision_score(y_test, preds_rf, average = 'weighted'), 3)\n",
        "rf_precision_ci_length = z_value * np.sqrt((rf_precision * (1 - rf_precision)) / y_test.shape[0])\n",
        "rf_precision_ci_lower = round((rf_precision - rf_precision_ci_length), 3)\n",
        "rf_precision_ci_upper = round((rf_precision + rf_precision_ci_length), 3)\n",
        "rf_precision_str = str(rf_precision) + ' (' + str(rf_precision_ci_lower) + ' - ' + str(rf_precision_ci_upper) + ')'\n",
        "\n",
        "rf_recall = round(recall_score(y_test, preds_rf, average = 'weighted'), 3)\n",
        "rf_recall_ci_length = z_value * np.sqrt((rf_recall * (1 - rf_recall)) / y_test.shape[0])\n",
        "rf_recall_ci_lower = round((rf_recall - rf_recall_ci_length), 3)\n",
        "rf_recall_ci_upper = round((rf_recall + rf_recall_ci_length), 3)\n",
        "rf_recall_str = str(rf_recall) + ' (' + str(rf_recall_ci_lower) + ' - ' + str(rf_recall_ci_upper) + ')'\n",
        "\n",
        "rf_auprc = round(average_precision_score(y_test, probs_rf, average = 'weighted'), 3)\n",
        "rf_auprc_ci_length = z_value * np.sqrt((rf_auprc * (1 - rf_auprc)) / y_test.shape[0])\n",
        "rf_auprc_ci_lower = round((rf_auprc - rf_auprc_ci_length), 3)\n",
        "rf_auprc_ci_upper = round((rf_auprc + rf_auprc_ci_length), 3)\n",
        "rf_auprc_str = str(rf_auprc) + ' (' + str(rf_auprc_ci_lower) + ' - ' + str(rf_auprc_ci_upper) + ')'\n",
        "\n",
        "rf_accuracy = round(balanced_accuracy_score(y_test, preds_rf), 3)\n",
        "rf_accuracy_ci_length = z_value * np.sqrt((rf_accuracy * (1 - rf_accuracy)) / y_test.shape[0])\n",
        "rf_accuracy_ci_lower = round((rf_accuracy - rf_accuracy_ci_length), 3)\n",
        "rf_accuracy_ci_upper = round((rf_accuracy + rf_accuracy_ci_length), 3)\n",
        "rf_accuracy_str = str(rf_accuracy) + ' (' + str(rf_accuracy_ci_lower) + ' - ' + str(rf_accuracy_ci_upper) + ')'\n",
        "\n",
        "rf_auroc, rf_auroc_ci_lower, rf_auroc_ci_upper = auroc_ci(y_test, probs_rf)\n",
        "rf_auroc = round(rf_auroc, 3)\n",
        "rf_auroc_ci_lower = round(rf_auroc_ci_lower, 3)\n",
        "rf_auroc_ci_upper = round(rf_auroc_ci_upper, 3)\n",
        "rf_auroc_str = str(rf_auroc) + ' (' + str(rf_auroc_ci_lower) + ' - ' + str(rf_auroc_ci_upper) + ')'\n",
        "\n",
        "rf_brier = round(brier_score_loss(y_test, probs_rf), 3)\n",
        "rf_brier_ci_length = z_value * np.sqrt((rf_brier * (1 - rf_brier)) / y_test.shape[0])\n",
        "rf_brier_ci_lower = round((rf_brier - rf_brier_ci_length), 3)\n",
        "rf_brier_ci_upper = round((rf_brier + rf_brier_ci_length), 3)\n",
        "rf_brier_str = str(rf_brier) + ' (' + str(rf_brier_ci_lower) + ' - ' + str(rf_brier_ci_upper) + ')'\n",
        "\n",
        "rf_results = [rf_precision_str, rf_recall_str, rf_auprc_str, rf_accuracy_str, rf_auroc_str, rf_brier_str]\n",
        "\n",
        "print(\"Precision: \", (rf_precision_str))\n",
        "print(\"Recall: \", (rf_recall_str))\n",
        "print('AUPRC: ', (rf_auprc_str))\n",
        "print('Accuracy: ', (rf_accuracy_str))\n",
        "print('AUROC: ', (rf_auroc_str))\n",
        "print('Brier Score: ', (rf_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Drrp3KEQIQPk"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "rf_precision_curve, rf_recall_curve, _ = precision_recall_curve(y_test, probs_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pypALF20IQPi"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX254OCwIQPj"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Logistic Regression.\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    param = {\n",
        "        \"penalty\": trial.suggest_categorical(\"penalty\", [\"l2\"]),\n",
        "        \"C\": trial.suggest_float(\"C\", 1e-4, 1e4, log=True),\n",
        "        \"fit_intercept\": trial.suggest_categorical(\"fit_intercept\", [True, False]),\n",
        "        \"solver\": trial.suggest_categorical(\"solver\", [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]),\n",
        "        \"max_iter\": trial.suggest_int(\"max_iter\", 100, 2000, 100),\n",
        "        \"random_state\": 31,\n",
        "    }\n",
        "\n",
        "    logreg = LogisticRegression(**param)\n",
        "\n",
        "    logreg.fit(\n",
        "        train_x,\n",
        "        train_y,\n",
        "    )\n",
        "\n",
        "    preds = logreg.predict(valid_x)\n",
        "    pred_labels = np.rint(preds)\n",
        "    auc = sklearn.metrics.roc_auc_score(valid_y, pred_labels)\n",
        "\n",
        "    return auc\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=31))\n",
        "    study.optimize(objective, n_trials=100, timeout=600)\n",
        "\n",
        "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))\n",
        "\n",
        "    logreg_params = {}\n",
        "\n",
        "    for key, value in trial.params.items():\n",
        "        logreg_params[key] = value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zq4CNR4JkE0y"
      },
      "outputs": [],
      "source": [
        "#See Logistic Regression hyperparameters.\n",
        "\n",
        "logreg_params['random_state'] = 31\n",
        "\n",
        "print(logreg_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk72VQghIQPj"
      },
      "outputs": [],
      "source": [
        "#Fit Logistic Regression.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(**logreg_params)\n",
        "\n",
        "logreg.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKJn1U-RIQPj"
      },
      "outputs": [],
      "source": [
        "#Predict on the validation set, get predicted probabilities for calibration, and fit calibration function.\n",
        "\n",
        "calib_probs_logreg = logreg.predict_proba(valid_x)\n",
        "\n",
        "calib_model_logreg = LogisticRegression()\n",
        "calib_model_logreg.fit(calib_probs_logreg, valid_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KSruYHE-QsT"
      },
      "outputs": [],
      "source": [
        "#Make predictions on the test set based on the trained and calibrated Logistic Regression model.\n",
        "\n",
        "preds_logreg = logreg.predict(x_test)\n",
        "\n",
        "uncalibrated_probs_logreg = logreg.predict_proba(x_test)\n",
        "uncalibrated_probs_logreg = uncalibrated_probs_logreg\n",
        "\n",
        "probs_logreg = calib_model_logreg.predict_proba(uncalibrated_probs_logreg)\n",
        "probs_logreg = probs_logreg[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTi-oHuXIQPj"
      },
      "outputs": [],
      "source": [
        "#Evaluate Logistic Regression model.\n",
        "\n",
        "logreg_precision = round(precision_score(y_test, preds_logreg, average = 'weighted'), 3)\n",
        "logreg_precision_ci_length = z_value * np.sqrt((logreg_precision * (1 - logreg_precision)) / y_test.shape[0])\n",
        "logreg_precision_ci_lower = round((logreg_precision - logreg_precision_ci_length), 3)\n",
        "logreg_precision_ci_upper = round((logreg_precision + logreg_precision_ci_length), 3)\n",
        "logreg_precision_str = str(logreg_precision) + ' (' + str(logreg_precision_ci_lower) + ' - ' + str(logreg_precision_ci_upper) + ')'\n",
        "\n",
        "logreg_recall = round(recall_score(y_test, preds_logreg, average = 'weighted'), 3)\n",
        "logreg_recall_ci_length = z_value * np.sqrt((logreg_recall * (1 - logreg_recall)) / y_test.shape[0])\n",
        "logreg_recall_ci_lower = round((logreg_recall - logreg_recall_ci_length), 3)\n",
        "logreg_recall_ci_upper = round((logreg_recall + logreg_recall_ci_length), 3)\n",
        "logreg_recall_str = str(logreg_recall) + ' (' + str(logreg_recall_ci_lower) + ' - ' + str(logreg_recall_ci_upper) + ')'\n",
        "\n",
        "logreg_auprc = round(average_precision_score(y_test, preds_logreg, average = 'weighted'), 3)\n",
        "logreg_auprc_ci_length = z_value * np.sqrt((logreg_auprc * (1 - logreg_auprc)) / y_test.shape[0])\n",
        "logreg_auprc_ci_lower = round((logreg_auprc - logreg_auprc_ci_length), 3)\n",
        "logreg_auprc_ci_upper = round((logreg_auprc + logreg_auprc_ci_length), 3)\n",
        "logreg_auprc_str = str(logreg_auprc) + ' (' + str(logreg_auprc_ci_lower) + ' - ' + str(logreg_auprc_ci_upper) + ')'\n",
        "\n",
        "logreg_accuracy = round(balanced_accuracy_score(y_test, preds_logreg), 3)\n",
        "logreg_accuracy_ci_length = z_value * np.sqrt((logreg_accuracy * (1 - logreg_accuracy)) / y_test.shape[0])\n",
        "logreg_accuracy_ci_lower = round((logreg_accuracy - logreg_accuracy_ci_length), 3)\n",
        "logreg_accuracy_ci_upper = round((logreg_accuracy + logreg_accuracy_ci_length), 3)\n",
        "logreg_accuracy_str = str(logreg_accuracy) + ' (' + str(logreg_accuracy_ci_lower) + ' - ' + str(logreg_accuracy_ci_upper) + ')'\n",
        "\n",
        "logreg_auroc, logreg_auroc_ci_lower, logreg_auroc_ci_upper = auroc_ci(y_test, probs_logreg)\n",
        "logreg_auroc = round(logreg_auroc, 3)\n",
        "logreg_auroc_ci_lower = round(logreg_auroc_ci_lower, 3)\n",
        "logreg_auroc_ci_upper = round(logreg_auroc_ci_upper, 3)\n",
        "logreg_auroc_str = str(logreg_auroc) + ' (' + str(logreg_auroc_ci_lower) + ' - ' + str(logreg_auroc_ci_upper) + ')'\n",
        "\n",
        "logreg_brier = round(brier_score_loss(y_test, probs_logreg), 3)\n",
        "logreg_brier_ci_length = z_value * np.sqrt((logreg_brier * (1 - logreg_brier)) / y_test.shape[0])\n",
        "logreg_brier_ci_lower = round((logreg_brier - logreg_brier_ci_length), 3)\n",
        "logreg_brier_ci_upper = round((logreg_brier + logreg_brier_ci_length), 3)\n",
        "logreg_brier_str = str(logreg_brier) + ' (' + str(logreg_brier_ci_lower) + ' - ' + str(logreg_brier_ci_upper) + ')'\n",
        "\n",
        "logreg_results = [logreg_precision_str, logreg_recall_str, logreg_auprc_str, logreg_accuracy_str, logreg_auroc_str, logreg_brier_str]\n",
        "\n",
        "print(\"Precision: \", (logreg_precision_str))\n",
        "print(\"Recall: \", (logreg_recall_str))\n",
        "print('AUPRC: ', (logreg_auprc_str))\n",
        "print('Accuracy: ', (logreg_accuracy_str))\n",
        "print('AUROC: ', (logreg_auroc_str))\n",
        "print('Brier Score: ', (logreg_brier_str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiuZD44hIQPj"
      },
      "outputs": [],
      "source": [
        "#Recalculate precision recall curve for plotting purposes.\n",
        "\n",
        "logreg_precision_curve, logreg_recall_curve, _ = precision_recall_curve(y_test, probs_logreg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLvNgketIQPl"
      },
      "source": [
        "# ROC and PR Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mMqO-euIQPl"
      },
      "outputs": [],
      "source": [
        "#Plot ROC curve.\n",
        "\n",
        "f = pyplot.figure()\n",
        "f.set_figwidth(12)\n",
        "f.set_figheight(12)\n",
        "\n",
        "tabpfn_fpr, tabpfn_tpr, _ = roc_curve(y_test, probs_tabpfn)\n",
        "tabpfn_label = 'TabPFN AUROC: ' + tabpfn_auroc_str\n",
        "pyplot.plot(tabpfn_fpr, tabpfn_tpr, label = tabpfn_label, color = 'b', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "tabnet_fpr, tabnet_tpr, _ = roc_curve(y_test, probs_tabnet)\n",
        "tabnet_label = 'TabNet AUROC: ' + tabnet_auroc_str\n",
        "pyplot.plot(tabnet_fpr, tabnet_tpr, label = tabnet_label, color = 'g', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, probs_xgb)\n",
        "xgb_label = 'XGBoost AUROC: ' + xgb_auroc_str\n",
        "pyplot.plot(xgb_fpr, xgb_tpr, label = xgb_label, color = 'r', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "lgb_fpr, lgb_tpr, _ = roc_curve(y_test, probs_lgb)\n",
        "lgb_label = 'LightGBM AUROC: ' + lgb_auroc_str\n",
        "pyplot.plot(lgb_fpr, lgb_tpr, label = lgb_label, color='c', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "rf_fpr, rf_tpr, _ = roc_curve(y_test, probs_rf)\n",
        "rf_label = 'Random Forest AUROC: ' + rf_auroc_str\n",
        "pyplot.plot(rf_fpr, rf_tpr, label = rf_label, color = 'm', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "logreg_fpr, logreg_tpr, _ = roc_curve(y_test, probs_logreg)\n",
        "logreg_label = 'Logistic Regression AUROC: ' + logreg_auroc_str\n",
        "pyplot.plot(logreg_fpr, logreg_tpr, label = logreg_label, color = 'y', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "pyplot.plot([0, 1], [0, 1], linestyle = '--', linewidth=2)\n",
        "\n",
        "pyplot.title('B', x = -0.075, y = 1.005, fontsize = 75, pad = 20)\n",
        "pyplot.xlabel('False Positive Rate', fontsize = 22, fontweight = 'heavy', labelpad = 16)\n",
        "pyplot.ylabel('True Positive Rate', fontsize = 22, fontweight = 'heavy', labelpad = 16)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 16)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 16)\n",
        "\n",
        "leg = pyplot.legend(loc = 'lower right', fontsize = 18)\n",
        "\n",
        "# pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_roc.png', dpi=300)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpotrYFBIQPl"
      },
      "outputs": [],
      "source": [
        "#Plot PR curve.\n",
        "\n",
        "f = pyplot.figure()\n",
        "f.set_figwidth(12)\n",
        "f.set_figheight(12)\n",
        "\n",
        "tabpfn_label = 'TabPFN AUPRC: ' + tabpfn_auprc_str\n",
        "tabnet_label = 'TabNet AUPRC: ' + tabnet_auprc_str\n",
        "xgb_label = 'XGBoost AUPRC: ' + xgb_auprc_str\n",
        "lgb_label = 'LightGBM AUPRC: ' + lgb_auprc_str\n",
        "rf_label = 'Random Forest AUPRC: ' + rf_auprc_str\n",
        "logreg_label = 'Logistic Regression AUPRC: ' + logreg_auprc_str\n",
        "\n",
        "pyplot.plot(tabpfn_recall_curve, tabpfn_precision_curve, label = tabpfn_label, color = 'b', linewidth = 3.5, alpha = 0.75)\n",
        "pyplot.plot(tabnet_recall_curve, tabnet_precision_curve, label = tabnet_label, color = 'g', linewidth = 3.5, alpha = 0.75)\n",
        "pyplot.plot(xgb_recall_curve, xgb_precision_curve, label = xgb_label, color = 'r', linewidth = 3.5, alpha = 0.75)\n",
        "pyplot.plot(lgb_recall_curve, lgb_precision_curve, label = lgb_label, color = 'c', linewidth = 3.5, alpha = 0.75)\n",
        "pyplot.plot(rf_recall_curve, rf_precision_curve, label = rf_label, color = 'm', linewidth = 3.5, alpha = 0.75)\n",
        "pyplot.plot(logreg_recall_curve, logreg_precision_curve, label = logreg_label, color = 'y', linewidth = 3.5, alpha = 0.75)\n",
        "\n",
        "pyplot.title('B', x = -0.075, y = 1.005, fontsize = 75, pad = 20)\n",
        "pyplot.xlabel('Recall', fontsize = 18, fontweight = 'heavy', labelpad = 16)\n",
        "pyplot.ylabel('Precision', fontsize = 22, fontweight = 'heavy', labelpad = 16)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 16)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 16)\n",
        "\n",
        "leg = pyplot.legend(loc = 'upper right', fontsize = 18)\n",
        "\n",
        "# pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_prc.png', dpi=300)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOySRdcDIQPl"
      },
      "source": [
        "# Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kznujF4kIQPl"
      },
      "outputs": [],
      "source": [
        "results = {'TabPFN':tabpfn_results, 'TabNet':tabnet_results, 'XGBoost':xgb_results, 'LightGBM':lgb_results, 'Random Forest':rf_results, 'Logistic Regression':logreg_results}\n",
        "\n",
        "results = pd.DataFrame(results, columns = ['TabPFN', 'TabNet', 'XGBoost', 'LightGBM', 'Random Forest', 'Logistic Regression'])\n",
        "results = results.T\n",
        "\n",
        "results.columns = ['Weighted Precision (95% CI)', 'Weighted Recall (95% CI)',  'Weighted AUPRC (95% CI)', 'Balanced Accuracy (95% CI)', 'AUROC (95% CI)', 'Brier Score']\n",
        "\n",
        "# results.to_csv('/content/drive/MyDrive/NSQIP-ALIF/discharge_results.csv')\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYnu4PnCIQPm"
      },
      "source": [
        "# SHAP Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuvljQiYu-Xb"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "def wrap_labels(ax, width, break_long_words=False):\n",
        "    labels = []\n",
        "    for label in ax.get_yticklabels():\n",
        "        text = label.get_text()\n",
        "        labels.append(textwrap.fill(text, width=width,\n",
        "                                    break_long_words=break_long_words))\n",
        "    ax.set_yticklabels(labels, rotation=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjL0ZhOZJh-b"
      },
      "outputs": [],
      "source": [
        "x_test_sample = x_test.sample(frac=0.25, random_state=42)\n",
        "\n",
        "feature_names = x_test_sample.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GTKndsLJzH7h"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for TabPFN.\n",
        "\n",
        "tabpfn_explainer = shap.Explainer(tabpfn.predict, x_test_sample.sample(frac=0.25, random_state=42))\n",
        "tabpfn_shap_values = tabpfn_explainer(x_test_sample.sample(frac=0.25, random_state=42))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Lw5zXxxQzH7p"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for TabPFN.\n",
        "\n",
        "shap.plots.bar(tabpfn_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('A', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_tabpfn.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XotWpfehXbCw"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for TabNet.\n",
        "\n",
        "tabnet_explainer = shap.Explainer(tabnet.predict, x_test_sample.values, feature_names=feature_names)\n",
        "tabnet_shap_values = tabnet_explainer(x_test_sample.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tzHasDCwXoxs"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for TabNet.\n",
        "\n",
        "shap.plots.bar(tabnet_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('B', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_tabnet.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C09esVZrIQPm"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for XGBoost.\n",
        "\n",
        "xgb_explainer = shap.Explainer(xgb.predict, x_test_sample)\n",
        "xgb_shap_values = xgb_explainer(x_test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6O74ff97qW2H"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for XGBoost.\n",
        "\n",
        "shap.plots.bar(xgb_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('C', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_xgb.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HBQmzC4-IQPm"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for LightGBM.\n",
        "\n",
        "lgb_explainer = shap.Explainer(lgb.predict, x_test_sample)\n",
        "lgb_shap_values = lgb_explainer(x_test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4V-NEfx_xysV"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for LightGBM.\n",
        "\n",
        "shap.plots.bar(lgb_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('B', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_lgb.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Vj3B1IeWIQPm"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for Random Forest.\n",
        "\n",
        "rf_explainer = shap.Explainer(rf.predict, x_test_sample)\n",
        "rf_shap_values = rf_explainer(x_test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2P2or1bx25u"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for Random Forest.\n",
        "\n",
        "shap.plots.bar(rf_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('D', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_rf.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GSP5OhIFNzCL"
      },
      "outputs": [],
      "source": [
        "#Calculate SHAP values for Logistic Regression.\n",
        "\n",
        "logreg_explainer = shap.Explainer(logreg.predict, x_test_sample)\n",
        "logreg_shap_values = logreg_explainer(x_test_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bt0NhytjN3e7"
      },
      "outputs": [],
      "source": [
        "#Plot SHAP bar plot for Logistic Regression.\n",
        "\n",
        "shap.plots.bar(logreg_shap_values, max_display = 15, show=False)\n",
        "\n",
        "fig = pyplot.gcf()\n",
        "ax = pyplot.gca()\n",
        "fig.set_figheight(12)\n",
        "fig.set_figwidth(5)\n",
        "\n",
        "pyplot.title('E', x = -0.5, y = 1, fontsize = 50, pad = 20)\n",
        "pyplot.xlabel(\"Mean |SHAP Value|\", fontsize =12, fontweight = 'heavy', labelpad = 8)\n",
        "pyplot.tick_params(axis=\"y\",direction=\"out\", labelsize = 12)\n",
        "pyplot.tick_params(axis=\"x\",direction=\"out\", labelsize = 12)\n",
        "\n",
        "wrap_labels(ax, 30)\n",
        "ax.figure\n",
        "\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_shap_logreg.png', dpi=300, bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwYwpeSZz_-x"
      },
      "source": [
        "#Partial Dependency Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cCezSsG3XK1c"
      },
      "outputs": [],
      "source": [
        "column_filters = {\n",
        "    'Age': (18, 90),\n",
        "    'Weight': (30, 250),\n",
        "    'Height': (100, 220),\n",
        "    'Preoperative Serum Sodium': (130, 150),\n",
        "    'Preoperative Serum BUN': (1, 100),\n",
        "    'Preoperative Serum Creatinine': (0, 5),\n",
        "    'Preoperative WBC Count': (0.5, 30),\n",
        "    'Preoperative Hematocrit': (30, 60),\n",
        "    'Preoperative Platelet Count': (50, 900),\n",
        "}\n",
        "\n",
        "for column, (min_val, max_val) in column_filters.items():\n",
        "    if column in x_test.columns:\n",
        "        x_test = x_test[(x_test[column] > min_val) & (x_test[column] < max_val)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jLWR5lmo01KM"
      },
      "outputs": [],
      "source": [
        "pyplot.rcParams[\"figure.figsize\"] = (15, 15)\n",
        "pyplot.rcParams[\"figure.dpi\"] = 300\n",
        "pyplot.rcParams['axes.labelweight'] = 'bold'\n",
        "pyplot.rcParams['axes.labelsize'] = 12\n",
        "pyplot.rcParams['axes.labelpad'] = 6\n",
        "pyplot.rcParams['font.weight'] = 'normal'\n",
        "pyplot.rcParams['lines.linewidth'] = 2.5\n",
        "pyplot.rcParams['xtick.labelsize'] = 8\n",
        "pyplot.rcParams['ytick.labelsize'] = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mCUeepoYdNJ"
      },
      "outputs": [],
      "source": [
        "categorical_features = ['Sex', 'Race', 'Hispanic Ethnicity', 'Transfer Status', 'Diabetes Mellitus Requiring Therapy', 'Current Smoker Status', 'Dyspnea', 'Ventilator Dependency', 'Functional Status', 'History of Severe COPD', 'CHF within 30 Days Prior to Surgery', 'Hypertension Requiring Medication', 'Acute Renal Failure', 'Currently Requiring or on Dialysis', 'Disseminated Cancer', 'Open Wound', 'Steroid or Immunosuppressant for a Chronic Condition', 'Malnourishment', 'Bleeding Disorder', 'RBC Transfusion within 72 Hours Prior to Surgery', 'ASA Classification', 'Single or Multiple Level Surgery', 'Surgical Specialty']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNB6VbOg5KKx"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "tabpfn_mean_abs_shap_values = np.mean(np.abs(tabpfn_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "tabpfn_shap_summary = pd.DataFrame(list(zip(feature_names, tabpfn_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "tabpfn_shap_summary_sorted = tabpfn_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "tabpfn_features = tabpfn_shap_summary_sorted['Feature'].tolist()\n",
        "tabpfn_features = tabpfn_features[:9]\n",
        "\n",
        "tabpfn_categorical_features = [item for item in tabpfn_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(tabpfn, x_test_sample.sample(frac=0.05, random_state=42), tabpfn_features, categorical_features = tabpfn_categorical_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_tabpfn.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLDT7TOZ5JFi"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "tabnet_mean_abs_shap_values = np.mean(np.abs(tabnet_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "tabnet_shap_summary = pd.DataFrame(list(zip(feature_names, tabnet_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "tabnet_shap_summary_sorted = tabnet_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "tabnet_features = tabnet_shap_summary_sorted['Feature'].tolist()\n",
        "tabnet_features = tabnet_features[:9]\n",
        "\n",
        "tabnet_categorical_features = [item for item in tabnet_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(xgb, x_test_sample, tabnet_features, categorical_features = tabnet_categorical_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_tabnet.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frN2YQUD0IWP"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "xgb_mean_abs_shap_values = np.mean(np.abs(xgb_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "xgb_shap_summary = pd.DataFrame(list(zip(feature_names, xgb_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "xgb_shap_summary_sorted = xgb_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "xgb_features = xgb_shap_summary_sorted['Feature'].tolist()\n",
        "xgb_features = xgb_features[:9]\n",
        "\n",
        "xgb_categorical_features = [item for item in xgb_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(xgb, x_test_sample, xgb_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_xgb.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9ZSSzpP5MbC"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "lgb_mean_abs_shap_values = np.mean(np.abs(lgb_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "lgb_shap_summary = pd.DataFrame(list(zip(feature_names, lgb_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "lgb_shap_summary_sorted = lgb_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "lgb_features = lgb_shap_summary_sorted['Feature'].tolist()\n",
        "lgb_features = lgb_features[:9]\n",
        "\n",
        "lgb_categorical_features = [item for item in lgb_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(lgb, x_test_sample, lgb_features, categorical_features = lgb_categorical_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_lgb.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_features"
      ],
      "metadata": {
        "id": "YSxi66gZmupt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq_m3WU_5NKq"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "rf_mean_abs_shap_values = np.mean(np.abs(rf_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "rf_shap_summary = pd.DataFrame(list(zip(feature_names, rf_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "rf_shap_summary_sorted = rf_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "rf_features = rf_shap_summary_sorted['Feature'].tolist()\n",
        "rf_features = rf_features[:9]\n",
        "\n",
        "rf_categorical_features = [item for item in rf_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(rf, x_test_sample, rf_features, categorical_features = rf_categorical_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_rf.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XEFBbQJOv_6"
      },
      "outputs": [],
      "source": [
        "#Calculate the mean absolute SHAP values for each feature.\n",
        "logreg_mean_abs_shap_values = np.mean(np.abs(logreg_shap_values.values), axis=0)\n",
        "\n",
        "#Create a DataFrame to map feature names to their mean absolute SHAP values.\n",
        "logreg_shap_summary = pd.DataFrame(list(zip(feature_names, logreg_mean_abs_shap_values)), columns=['Feature', 'Mean SHAP'])\n",
        "\n",
        "#Sort the DataFrame by 'Mean SHAP' in descending order.\n",
        "logreg_shap_summary_sorted = logreg_shap_summary.sort_values('Mean SHAP', ascending=False)\n",
        "\n",
        "#Get the names of the features.\n",
        "logreg_features = logreg_shap_summary_sorted['Feature'].tolist()\n",
        "logreg_features = logreg_features[:9]\n",
        "\n",
        "logreg_categorical_features = [item for item in logreg_features if item in categorical_features]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(logreg, x_test_sample, logreg_features, categorical_features = logreg_categorical_features)\n",
        "pyplot.savefig('/content/drive/MyDrive/NSQIP-ALIF/discharge_pdp_logreg.png', dpi=300, bbox_inches='tight')\n",
        "pyplot.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}